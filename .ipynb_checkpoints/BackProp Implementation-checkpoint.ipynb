{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation, equations, and symbols were derived from https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/. These equations were derived in our last session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_j$: input to node $j$ for layer $l$\n",
    "\n",
    "$g_j$: activation function for node $j$ in layer $l$ (applied to $z_j$)\n",
    "\n",
    "$a_j = g_j(z_j)$: output/activation of node $j$ in layer $l$\n",
    "\n",
    "$w_{i,j}$: weights connecting node $i$ in layer $(l-1)$ to node $j$ in layer $l$\n",
    "\n",
    "$t_k$: target value for node $k$ in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Output Layer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta_k: (a_k-t_k)g_k'(z_k)$\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{w_{j,k}}}$: $\\delta_ka_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation was really confusing so I had to break it down line by line. When I am looking at the output layer, then my $a_k$ is the output of the node of the output layer and $a_j$ is the output from my hidden layer. $t_k$ is my target variable and $g_k'(z_k) = a_k'$ the derivative of the output layer. The change variable is what I will use for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Hidden Layer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta_j = g_j'(z_j)\\sum_k^K \\delta_kw_{j,k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $j$ represents the hidden layer that the weight is originating from and $k$ represents the output layer, where the weight is pointing to. In the case of a 1 hidden layer model, $j$ is the hidden layer and $k$ is the output layer. You want to sum over all of the output nodes that this weight could point to because when you do backprop, all of these nodes are affected by a change in this weight. $\\delta_k$ is the delta for the kth node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN_backprop:\n",
    "    def __init__(self, n_input, n_hidden,n_nodes):\n",
    "        self.n_input = n_input + 1\n",
    "        self.n_layers = n_hidden +2\n",
    "        self.n_nodes = n_nodes\n",
    "        \n",
    "        self.a_layers = np.ones(self.n_layers, self.n_nodes)\n",
    "        self.z_output = np.ones((self.n_layers, self.n_nodes))\n",
    "        self.a_output = np.ones((self.n_layers, self.n_nodes))\n",
    "        \n",
    "        self.delta = np.ones((self.n_layers, self.n_nodes))\n",
    "        \n",
    "        self.w = np.random.rand((self.n_layers, self.n_nodes, self.n_nodes))\n",
    "        \n",
    "        # stores the gradients to update it\n",
    "        self.c_ = np.random.rand((self.n_layers, self.n_nodes, self.n_nodes))\n",
    "  \n",
    "    def runNN (self, inputs):\n",
    "        self.a_input = inputs\n",
    "        self.layers[0] = inputs\n",
    "        \n",
    "        # basically doing sigmoid(w^1x+w^1_0) to find output for each hidden layer\n",
    "        for h in range(1,self.layers):\n",
    "            for z_ in range(self.n_nodes):\n",
    "                self.z_output[h][z_] = np.sum(np.dot(self.a_output[h-1],self.w[h,z_,]))\n",
    "                self.a_hidden[h][z_] = self.sigmoid(self.z_output[h][z_])  \n",
    "                                              \n",
    "        return self.a_output\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "    \n",
    "    def derivative_sigmoid(x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def backPropagate (self, targets, eta):\n",
    "    \n",
    "    # https://theclevermachine.wordpress.com/2014/09/06/\n",
    "    #derivation-error-backpropagation-gradient-descent-for-neural-networks/\n",
    "    \n",
    "        ### dE/dW_jk\n",
    "        \n",
    "        output_deltas = np.zeros(self.n_nodes)\n",
    "        self.delta[-1] = (self.a_output[-1]-targets)*derivative_sigmoid(self.z_output[-1])\n",
    "        self.c_[-1,:,:] = self.delta[-1]*self.a_output[-2]\n",
    "        self.w[-1,:,:] -= eta*self.c_[-1,:,:]\n",
    "                                         \n",
    "        #output delta should be a k x 1 array and self.a_hidden is a 1 x n_hidden\n",
    "        # so c_output should be a n_output x n_hidden or something along these lines\n",
    "        \n",
    "        # these hodl the gradient changes for the weights that go from hidden to ouput\n",
    "        for h in xrange(self.n_layers,-1):\n",
    "            for i in xrange(self.n_nodes):\n",
    "                for j in xrange(self.n_nodes):\n",
    "                    for k in xrange(self.n_nodes):\n",
    "                        self.delta[h][j] += derivative_sigmoid(self.z_output[h][j])*self.delta[h][k]*self.w[h,j,k]\n",
    "\n",
    "                        # these hold the gradient changes for the weights that go from input to hidden\n",
    "                        self.c_input[h,i,j] = self.delta[h][j]*self.a_output[h][i]\n",
    "\n",
    "                        #update the weights\n",
    "                        self.w[h,i,j] -= eta*self.c_input[h,i,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pat = [\n",
    "      [[0,0], [1]],\n",
    "      [[0,1], [1]],\n",
    "      [[1,0], [1]],\n",
    "      [[1,1], [0]]\n",
    "  ]\n",
    "myNN = NN_backprop( 2, 2, 1)\n",
    "inputs = pat[0]\n",
    "targets = pat[1]\n",
    "myNN.runNN(pat)\n",
    "# myNN.backPropagate(targets, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3,5,4))[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
